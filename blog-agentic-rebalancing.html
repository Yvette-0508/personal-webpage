<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building a Financial Research Agent: Graph + RAG + Continuous Evaluation - Yan Pan</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-brand">YAN PAN</div>
            <div class="nav-links">
                <a href="index.html">About</a>
                <a href="resume.html">Resume</a>
                <a href="portfolio.html">Portfolio</a>
                <a href="blog.html" class="active">Blog</a>
                <a href="reading.html">Reading</a>
            </div>
        </div>
    </nav>

    <main class="container blog-post-page">
        <article class="blog-article">
            <header class="blog-article-header">
                <h1 class="blog-article-title">Building a Financial Research Agent: Graph + RAG + Continuous Evaluation</h1>
                <div class="blog-post-meta">
                    <span class="blog-post-date">November 26, 2024</span>
                    <span class="blog-post-category">AI Agents</span>
                    <span class="blog-post-category">Quantitative Finance</span>
                    <span class="blog-post-category">RAG</span>
                </div>
            </header>

            <div class="blog-article-content">
                <h2>The Problem: Information Overload in Investment Research</h2>
                <p>
                    As a quantamental investor managing a multi-asset portfolio, I constantly synthesize information 
                    from disparate sources: proprietary research notes, SEC filings, earnings transcripts, real-time 
                    market news, and sector analysis. The challenge isn't finding information—it's finding the 
                    <em>right</em> information at the <em>right</em> time and reasoning about it coherently.
                </p>
                <p>
                    Traditional search fails here. Keyword matching misses semantic intent. Manual research doesn't 
                    scale. I needed a system that could:
                </p>
                <ul>
                    <li><strong>Store and retrieve</strong> my proprietary research semantically</li>
                    <li><strong>Augment</strong> with real-time web intelligence</li>
                    <li><strong>Reason</strong> across both sources to generate actionable insights</li>
                </ul>

                <h2>The Solution: A Financial Research Agent (Modular Stack)</h2>
                <p>
                    The core idea is stable (tool-augmented retrieval + grounded synthesis), but the <em>stack evolves</em> as providers change.
                    In the current codebase, the agent is a tool-calling loop with context management, backed by multiple stores (SQL + graph + vectors + cache)
                    and a continuous evaluation suite that prevents silent regressions.
                </p>
                <table class="tech-table">
                    <thead>
                        <tr>
                            <th>Component</th>
                            <th>Implementation</th>
                            <th>Purpose</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>LLM runtime</strong></td>
                            <td>LiteLLM (OpenAI-compatible tool calling)</td>
                            <td>Reasoning + tool orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>Embeddings</strong></td>
                            <td>Voyage AI finance embeddings (e.g., <code>voyage-finance-2</code>)</td>
                            <td>Semantic retrieval over filings and extracted report chunks</td>
                        </tr>
                        <tr>
                            <td><strong>Vector store</strong></td>
                            <td>Qdrant (vector search + metadata filters)</td>
                            <td>Fast retrieval with company/report/section/period filtering</td>
                        </tr>
                        <tr>
                            <td><strong>Web intelligence</strong></td>
                            <td>Exa (domain-filtered news / research / filings)</td>
                            <td>Fresh context: news, SEC filings, research</td>
                        </tr>
                        <tr>
                            <td><strong>Knowledge graph</strong></td>
                            <td>Neo4j (supply-chain graph)</td>
                            <td>Structured dependency queries and impact analysis</td>
                        </tr>
                        <tr>
                            <td><strong>Relational store</strong></td>
                            <td>Postgres (portfolio / positions)</td>
                            <td>Portfolio state and risk inputs</td>
                        </tr>
                        <tr>
                            <td><strong>Cache</strong></td>
                            <td>Redis (tool result caching)</td>
                            <td>Lower latency and more stable unit economics</td>
                        </tr>
                    </tbody>
                </table>

                <h2>Evaluation Is the Product</h2>
                <p>
                    In investment workflows, a “good demo” is not success. A system is only useful if it is consistently correct, properly sourced,
                    and stable under distribution shift. I treated evaluation as a first-class feature: every change to prompts, tools, retrieval,
                    or model routing must move measurable metrics in the right direction.
                </p>
                <table class="tech-table">
                    <thead>
                        <tr>
                            <th>What I evaluate</th>
                            <th>How it fails in practice</th>
                            <th>What I measure</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Tool selection</strong></td>
                            <td>Calls the wrong tools, skips required tools, or loops</td>
                            <td>Expected tools vs actual tool calls (overlap/Jaccard)</td>
                        </tr>
                        <tr>
                            <td><strong>Entity grounding</strong></td>
                            <td>Drifts to the wrong ticker/supplier, mixes entities</td>
                            <td>Expected entities present; prohibited entities absent</td>
                        </tr>
                        <tr>
                            <td><strong>Numeric accuracy</strong></td>
                            <td>Wrong numbers, unit mistakes, extraction bugs</td>
                            <td>Expected values within tolerance (robust numeric parsing)</td>
                        </tr>
                        <tr>
                            <td><strong>Hallucinations</strong></td>
                            <td>Fabricated facts and confident nonsense</td>
                            <td>Must-not-contain constraints + factual checks</td>
                        </tr>
                        <tr>
                            <td><strong>Difficulty tiers</strong></td>
                            <td>Easy QA looks fine; strategy fails under pressure</td>
                            <td>Easy/Medium/Hard thresholds (factual → reasoning → strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>Latency + cost</strong></td>
                            <td>Slow responses or unsustainable spend</td>
                            <td>Response time + tokens per turn</td>
                        </tr>
                    </tbody>
                </table>

                <h2>Architecture</h2>
<pre class="code-block">
┌─────────────────────────────────────────────────────────────────┐
│                         Agent Layer                              │
│      Router / Planner / Executor + Context Manager (tool calls)   │
└─────────────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────────────┐
│                          Tool Layer                              │
│  Portfolio Tools | Supply Chain Tools | Report Tools | Exa Search │
└─────────────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────────────┐
│                         Storage Layer                             │
│     Postgres (portfolio) | Neo4j (graph) | Qdrant (vectors)       │
│                      Redis (cache)                                 │
└─────────────────────────────────────────────────────────────────┘
</pre>

                <h2>Why This Design?</h2>
                <p>
                    Vendor names are less important than the design constraints: retrieval quality, provenance/citations, latency/cost, and operational
                    reliability. I pick providers that are strong on financial text, offer predictable latency, and can be evaluated and monitored.
                </p>

                <h3>Agent loop (LiteLLM + tool calling)</h3>
                <p>
                    The repo implements a loop-based agent: the model proposes tool calls, tools execute, results go back into context, and the agent
                    synthesizes a final answer. Using LiteLLM keeps the interface OpenAI-compatible so the underlying model can change without rewriting the loop.
                </p>
<pre class="code-block">
from agent.main import FinancialResearchAgent

agent = FinancialResearchAgent(model=\"gpt-4o-mini\")  # any OpenAI-compatible model via LiteLLM
response = await agent.chat(\"Give me a morning briefing on my portfolio\")
print(response.content)
</pre>

                <h3>Vector retrieval (Voyage finance embeddings + Qdrant filters)</h3>
                <p>
                    Report chunks are embedded with a finance-specific model (query/document modes) and stored in Qdrant with indexed payload fields
                    like <code>company</code>, <code>report_type</code>, <code>section</code>, and <code>period</code>. This makes retrieval precise and testable.
                </p>
<pre class="code-block">
from storage.vector_store import VectorStore

vs = VectorStore()
await vs.init_collections()
results = await vs.search_reports(
    query=\"Blackwell demand commentary\",
    companies=[\"NVDA\"],
    report_types=[\"earnings_call\"],
    limit=5,
)
for r in results:
    print(r.score, r.chunk.section, r.chunk.period)
</pre>

                <h3>Freshness (Exa web search with domain filters)</h3>
                <p>
                    Web search is the “recency layer.” Exa is configured with domain allowlists for high-quality sources (news, research, SEC),
                    plus utilities like full-article content fetching when deeper reading is needed.
                </p>
<pre class="code-block">
from tools.web_search import ExaTools

exa = ExaTools()
news = await exa.search_financial_news(\"NVDA export restrictions\", days_back=7, num_results=5)
print(news)
</pre>

                <h3>Structured reasoning (Neo4j supply-chain graph + portfolio state)</h3>
                <p>
                    The differentiator vs. pure RAG is structured context: supply-chain relationships live in Neo4j and portfolio state lives in Postgres.
                    This enables dependency queries (“who supplies who”) and impact analysis (“what breaks if CoWoS is constrained”) that are hard to do with text alone.
                </p>

<pre class="code-block">
# In practice, the agent decides which tools to call (reports vs web vs graph vs portfolio)
# based on the query and the available context.
from agent.main import FinancialResearchAgent

agent = FinancialResearchAgent(model="gpt-4o-mini")
response = await agent.chat("What are the key risks for AI chip stocks right now?")
print(response.content)
</pre>

                <h2>Implementation Details</h2>

                <h3>Project Structure</h3>
<pre class="code-block">
financial-deep-research/
├── agent/                # Agent core loop + context manager
├── tools/                # Portfolio / supply chain / reports / Exa web search
├── storage/              # Postgres / Neo4j / Qdrant / Redis
├── ingestion/            # Report processing + graph building
├── evaluation/           # Datasets + metrics + runner
├── scripts/              # init_databases.py, run_evaluation.py
├── frontend/             # Optional UI
└── requirements.txt
</pre>

                <h3>Configuration</h3>
<pre class="code-block">
# .env
OPENAI_API_KEY=your_key
VOYAGE_API_KEY=your_key
EXA_API_KEY=your_key
QDRANT_URL=http://localhost:6333
NEO4J_URI=bolt://localhost:7687
POSTGRES_DSN=postgresql://user:pass@localhost:5432/db
REDIS_URL=redis://localhost:6379/0
</pre>

                <h3>CLI Usage</h3>
<pre class="code-block">
# Start agent (CLI loop)
python -m agent.main

# Initialize databases / collections
python scripts/init_databases.py

# Run evaluation suite (difficulty + category filters)
python scripts/run_evaluation.py --difficulty easy
python scripts/run_evaluation.py --difficulty hard --category investment_strategy --max-cases 5
</pre>

                <h2>What This System Enables</h2>
                <p>
                    Everything below maps directly to modules in the repo (tools + storage + evaluation):
                </p>
                <ul>
                    <li><strong>Portfolio Q&amp;A:</strong> query positions, exposure, and risk from Postgres-backed portfolio tools</li>
                    <li><strong>Supply-chain analysis:</strong> retrieve dependencies and run impact queries from the Neo4j knowledge graph</li>
                    <li><strong>Report-grounded answers:</strong> semantic search over filings/transcripts via Voyage embeddings + Qdrant filters</li>
                    <li><strong>Freshness:</strong> pull recent news/filings/research via Exa with domain filters</li>
                    <li><strong>Repeatable iteration:</strong> run the evaluation suite to catch regressions in tool selection, numeric accuracy, and hallucinations</li>
                </ul>

                <h2>How I Evaluate It (Practical Checklist)</h2>
                <p>
                    The most important thing I learned: tool-augmented RAG systems fail quietly. A correct-sounding answer can be based on the wrong tool output
                    or a mis-grounded number. My evaluation loop is built around explicit test cases with expected tools/entities/values and difficulty tiers.
                </p>
                <h3>1) Build a gold set of questions</h3>
                <p>
                    I maintain a benchmark suite of recurring investor workflows (supply-chain dependency, portfolio risk, report extraction, and strategic decisions).
                    Each case specifies what “correct behavior” means: expected tools, expected entities, expected numbers within tolerance, and what must not appear.
                </p>
<pre class="code-block"># eval/questions.json (illustrative)
[
  {
    "id": "nvda_risks_2024q4",
    "question": "What are the near-term risks for NVDA given current valuation and export controls?",
    "expected_tools": ["search_financial_news", "search_reports"],
    "expected_entities": ["NVDA"],
    "must_not_contain": ["fabricated numbers"]
  }
]</pre>

                <h3>2) Retrieval-first metrics</h3>
                <p>
                    Before judging the answer, I evaluate behavior: did the agent call the right tools and anchor to the right entities and numbers?
                    If tool selection is wrong, “better prompting” only hides the failure mode.
                </p>
                <ul>
                    <li><strong>Tool selection score:</strong> expected tools vs actual tool calls.</li>
                    <li><strong>Entity grounding:</strong> expected entities present; prohibited entities absent.</li>
                    <li><strong>Numeric correctness:</strong> extracted values within tolerance (and no “10-K → 10” parsing bugs).</li>
                </ul>

                <h3>3) Source-grounded answer scoring</h3>
                <p>
                    For answers, I bias toward grounded outputs. When the agent is missing data, it should ask for missing context or avoid hard claims
                    rather than hallucinating.
                </p>
                <ul>
                    <li><strong>Factual checks:</strong> expected facts present, prohibited claims absent.</li>
                    <li><strong>Hallucination detection:</strong> enforce must-not-contain constraints for known failure modes.</li>
                    <li><strong>Reasoning/strategy signals:</strong> for medium/hard cases, verify it provides analysis and actionable recommendations.</li>
                </ul>

                <h3>4) Latency and token economics</h3>
                <p>
                    For daily usage, cost and responsiveness are product features. I track latency percentiles and tokens-per-answer, and I use
                    routing (cheaper models for summarization, stronger models for synthesis) to hit budgets without collapsing quality.
                </p>

                <h3>5) Monitoring for drift</h3>
                <p>
                    Markets change and sources change. I log query types, retrieval distributions (doc types, dates), and “answer confidence”
                    proxies (abstentions, citation density) to detect drift early and refresh the benchmark set.
                </p>

                <p>
                    In practice, I run this continuously during iteration:
                </p>
<pre class="code-block">python scripts/run_evaluation.py --difficulty easy
python scripts/run_evaluation.py --difficulty hard --category investment_strategy --max-cases 5
</pre>

                <h2>Try It Yourself</h2>
                <p>The complete implementation is available at: <code>github.com/Yvette-0508/financial-deep-research</code> (private)</p>
<pre class="code-block">
git clone https://github.com/Yvette-0508/financial-deep-research
cd financial-deep-research
pip install -r requirements.txt
# Add your API keys to .env
python scripts/init_databases.py
python -m agent.main
</pre>

                <div class="blog-article-footer">
                    <p><a href="blog.html">← Back to Blog</a></p>
                </div>
            </div>
        </article>
    </main>

    <footer class="footer">
        <div class="footer-content">
            <p>© 2025 Yan Pan. Building intelligent trading systems.</p>
            <p class="contact-links">
                <a href="mailto:yanpan.0508@gmail.com">yanpan.0508@gmail.com</a>
                <span class="contact-sep">•</span>
                <a href="https://github.com/Yvette-0508" target="_blank" rel="noopener noreferrer">Git</a>
                <span class="contact-sep">•</span>
                <a href="https://www.linkedin.com/in/yvette-pan-488247173/" target="_blank" rel="noopener noreferrer">LinkedIn</a>
            </p>
        </div>
    </footer>
</body>
</html>
